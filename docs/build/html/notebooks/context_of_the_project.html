<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Real and Fake Face Detection (RFFD) &mdash; Fake and Real Face Detection with ViT 0.0.1 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Bayesian Optimization from Scratch üîù" href="bayesian_optimization.html" />
    <link rel="prev" title="Usage" href="../usage.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            Fake and Real Face Detection with ViT
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../usage.html">Usage</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Real and Fake Face Detection (RFFD)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#Objective">Objective</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Steps-üßæ">Steps üßæ</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="bayesian_optimization.html">Bayesian Optimization from Scratch üîù</a></li>
<li class="toctree-l1"><a class="reference internal" href="generate_and_visualize.html">Generation and Exploration of the Images üîé</a></li>
<li class="toctree-l1"><a class="reference internal" href="preprocessing_and_loading.html">Preprocessing and loading ‚öóÔ∏è</a></li>
<li class="toctree-l1"><a class="reference internal" href="split_dataset.html">Split data ü´ß</a></li>
<li class="toctree-l1"><a class="reference internal" href="vit_model.html">Vision Transformer Model + Configuration and Metrics üëì</a></li>
<li class="toctree-l1"><a class="reference internal" href="deploy_to_hugging_face.html">Deploy the ViT Model üöÄ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../package/fake_face_detection.html">fake_face_detection package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../package/fake_face_detection.data.html">fake_face_detection.data package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../package/fake_face_detection.metrics.html">fake_face_detection.metrics package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../package/fake_face_detection.optimization.html">fake_face_detection.optimization package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../package/fake_face_detection.trainers.html">fake_face_detection.trainers package</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Fake and Real Face Detection with ViT</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Real and Fake Face Detection (RFFD)</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/notebooks/context_of_the_project.ipynb.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="Real-and-Fake-Face-Detection-(RFFD)">
<h1>Real and Fake Face Detection (RFFD)<a class="headerlink" href="#Real-and-Fake-Face-Detection-(RFFD)" title="Permalink to this heading">ÔÉÅ</a></h1>
<p style="text-align: justify"><p>In this project, we use Deep Neural Networks to identify which image is fake or real. The training will be done on a dataset that we got from Kaggle (check it here kaggle_real_fake_faces) created by Seonghyeon Nam, Seoung Wug Oh, et al. They used expert knowledge to photoshop authentic images. The fake images range between easy, medium, or hard to recognize. The modifications are made on the eyes, nose, and mouth (which permit human beings to recognize others) or the whole face.</p>
</p><img alt="fake_photoshop" src="https://github.com/minostauros/Real-and-Fake-Face-Detection/raw/master/filename_description.jpg" />
<p>The above image was got from the kaggle description of the image and describe a file.</p>
<p>The image above is described as a fake image file. The name of the file can be decomposed into three different parts separated by underscores:</p>
<ul class="simple">
<li><p>The first part indicates the quality of the Photoshop or the difficulty of recognizing that it is fake;</p></li>
<li><p>The second part indicates the identification number of the image;</p></li>
<li><p>The third and final part indicates the modified segment of the face in binary digits with the following signature -&gt; <span class="math notranslate nohighlight">\(\color{orange}[bit\_left\_eye,\space bit\_right\_eye,\space bit\_nose,\space bit\_mouth]\)</span>.</p></li>
</ul>
<p>The segment is modified if it is the positive bit (1). Otherwise, the segment is not modified.</p>
<p>Let us define below the main parts of our project:</p>
<section id="Objective">
<h2>Objective<a class="headerlink" href="#Objective" title="Permalink to this heading">ÔÉÅ</a></h2>
<p>The project aims to use <code class="docutils literal notranslate"><span class="pre">Vision</span> <span class="pre">Transformer</span> <span class="pre">(ViT)</span></code> mixed with <code class="docutils literal notranslate"><span class="pre">Transfer</span> <span class="pre">Learning</span></code> to achieve great accuracy and recall on the validation set. ViT is a new field that tries to reproduce the same performance that the Convolution Neural Networks on image classification tasks but using the Transformer architecture. It can provide very accurate results.</p>
<img alt="VISION_TRANSFORMER" src="https://www.researchgate.net/profile/Jacob-Heilmann-Clausen/publication/357885173/figure/fig1/AS:1113907477389319&#64;1642587646516/Vision-Transformer-architecture-main-blocks-First-image-is-split-into-fixed-size_W640.jpg" />
<p>However, we cannot obtain such great results with only a few images. ViT requires around 14 million images to learn image classification tasks, and we want to train the model only on one GPU device. Then the solution is to use Transfer Learning with a pre-trained Transformer to improve the overall performance.</p>
<p>We will fine-tune the pre-trained ViT Model for which the ArXiv paper is available at the following link <a class="reference external" href="https://arxiv.org/pdf/2010.11929">Vision Transformer</a>. It was pre-trained on the ImageNet-21k, which contains 14 million images distributed over 21 thousand classes. The model is available in HuggingFace and can be imported with the HuggingFace API.</p>
<p>For the moment, we want to obtain the following scores on the validation set:</p>
<ul class="simple">
<li><p><strong>Accuracy &gt; 80</strong></p></li>
<li><p><strong>f1 &gt; 80</strong></p></li>
</ul>
<p>Since the predictions are only sometimes damaging if False, we will only force the model to obtain up to 90% of <strong>Accuracy</strong> and <strong>f1-score</strong>.</p>
<p>The following section describes the steps that are required to achieve the project.</p>
</section>
<section id="Steps-üßæ">
<h2>Steps üßæ<a class="headerlink" href="#Steps-üßæ" title="Permalink to this heading">ÔÉÅ</a></h2>
<ul class="simple">
<li><p>Data generation and Exploration: We must recuperate the images, visualize them, and identify their statistics. Moreover, we will define the augmentation methods to add to the pictures ‚û°Ô∏è <a class="reference internal" href="generate_and_visualize.html"><span class="doc">Generating_and_visualizing</span></a></p></li>
<li><p>Preprocessing method: We must, after Exploration, define the preprocessing to add before training the model on them. ‚û°Ô∏è <a class="reference internal" href="preprocessing_and_loading.html"><span class="doc">Preprocessing_and_loading</span></a></p></li>
<li><p>Split the images between train, validation, and test sets. ‚û°Ô∏è <a class="reference internal" href="split_dataset.html"><span class="doc">Data_splitting</span></a></p></li>
<li><p>Load the ViT Model, explain the architecture briefly, and define the metrics to add. ‚û°Ô∏è <a class="reference internal" href="vit_model.html"><span class="doc">VitModel_Metrics</span></a></p></li>
<li><p>Search for the best model with The Bayesian Optimization strategy. ‚û°Ô∏è <a class="reference external" href="best_model_search.ipynb">Search_best_model</a></p></li>
<li><p>Fine-tune the best model. üõë</p></li>
<li><p>Evaluate the model on the test set. üõë</p></li>
<li><p>Deploy the model to Hugging Face. ‚û°Ô∏è <a class="reference internal" href="deploy_to_hugging_face.html"><span class="doc">Deployment</span></a></p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../usage.html" class="btn btn-neutral float-left" title="Usage" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="bayesian_optimization.html" class="btn btn-neutral float-right" title="Bayesian Optimization from Scratch üîù" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, Oumar Kane.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>