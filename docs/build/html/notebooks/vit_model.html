<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Vision Transformer Model + Configuration and Metrics üëì &mdash; Fake and Real Face Detection with ViT 0.0.1 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/nbsphinx-code-cells.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Deploy the ViT Model üöÄ" href="deploy_to_hugging_face.html" />
    <link rel="prev" title="Split data ü´ß" href="split_dataset.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            Fake and Real Face Detection with ViT
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../usage.html">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="context_of_the_project.html">Real and Fake Face Detection (RFFD)</a></li>
<li class="toctree-l1"><a class="reference internal" href="bayesian_optimization.html">Bayesian Optimization from Scratch üîù</a></li>
<li class="toctree-l1"><a class="reference internal" href="generate_and_visualize.html">Generation and Exploration of the Images üîé</a></li>
<li class="toctree-l1"><a class="reference internal" href="preprocessing_and_loading.html">Preprocessing and loading ‚öóÔ∏è</a></li>
<li class="toctree-l1"><a class="reference internal" href="split_dataset.html">Split data ü´ß</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Vision Transformer Model + Configuration and Metrics üëì</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#Architecture">Architecture</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Attention">Attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Metrics-and-predictions">Metrics and predictions</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="deploy_to_hugging_face.html">Deploy the ViT Model üöÄ</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Fake and Real Face Detection with ViT</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Vision Transformer Model + Configuration and Metrics üëì</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/notebooks/vit_model.ipynb.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="Vision-Transformer-Model-+-Configuration-and-Metrics-üëì">
<h1>Vision Transformer Model + Configuration and Metrics üëì<a class="headerlink" href="#Vision-Transformer-Model-+-Configuration-and-Metrics-üëì" title="Permalink to this heading">ÔÉÅ</a></h1>
<p>The ViT Model is based on the architecture of the Transformer introduced by Vaswani et al. in the article <em>Attention Is All You Need</em>. While the original Transformer is commonly used for NLP tasks, the ViT Model was implemented to work on images like with Convolution Neural Networks. The Transformer initially took a sequence of vectors converted to an embedding vector of fixed features‚Äô size before being added to a 1-dimensional positional vector which learned to identify the position of each
element in the sequence. In the ViT Model, the series comprises the patches we define in the following notebook <a class="reference internal" href="preprocessing_and_loading.html"><span class="doc">preprocessing_and_loading</span></a>. Each patch must be flattened for we obtain an input sequence of size <span class="math notranslate nohighlight">\((N\times (D^2.C))\)</span>, where <span class="math notranslate nohighlight">\(N\)</span> is the number of patches, D is the size of the height and width of the patches, and <span class="math notranslate nohighlight">\(C\)</span> is the number of channels.</p>
<p>To make transfer learning with the pre-trained model, the classification layer composed of two linear layers and a GeLU activation will take as enter the output of the encoder reshaped to <span class="math notranslate nohighlight">\((batch\_size, Sequence\_size \times Model size)\)</span> to give us a final output of <span class="math notranslate nohighlight">\((batch\_size, number\_of\_labels)\)</span>. Only the weights of the classifier are randomly initialized to be trained on new images.</p>
<p>Let us see the architecture and define what makes it different from the original Transformer architecture.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>

<span class="c1"># importing some libraries</span>
<span class="kn">from</span> <span class="nn">fake_face_detection.data.fake_face_dataset</span> <span class="kn">import</span> <span class="n">FakeFaceDetectionDataset</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">ViTForImageClassification</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">ViTFeatureExtractor</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">transforms</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">pytorch_lightning</span> <span class="k">as</span> <span class="nn">pl</span>
<span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">evaluate</span>
<span class="kn">import</span> <span class="nn">pickle</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="c1"># set a seed for all the following process</span>
<span class="n">pl</span><span class="o">.</span><span class="n">seed_everything</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<br/></pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
Global seed set to 0
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
0
</pre></div></div>
</div>
<section id="Architecture">
<h2>Architecture<a class="headerlink" href="#Architecture" title="Permalink to this heading">ÔÉÅ</a></h2>
<p>The path or name of the ViT Model must be indicated to be loaded. We want to recuperate the base model pre-trained by Google on ImageNet21k‚Äôs images of size <span class="math notranslate nohighlight">\((224\times 224)\)</span> and requiring <span class="math notranslate nohighlight">\(16\)</span> patches. We need to indicate the following name: <code class="docutils literal notranslate"><span class="pre">google/vit-base-patch16-224-in21k</span></code>. We will also provide the number of labels which is <code class="docutils literal notranslate"><span class="pre">2</span></code> in our case, and we need to give the class Ids that we recuperated in <a class="reference internal" href="generate_and_visualize.html"><span class="doc">generate_an_visualize</span></a>.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_name</span> <span class="o">=</span> <span class="s1">&#39;google/vit-base-patch16-224-in21k&#39;</span>

<span class="c1"># recuperate the images characteristics</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;data/extractions/fake_real_dict.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>

    <span class="n">depick</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">Unpickler</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>

    <span class="n">characs</span> <span class="o">=</span> <span class="n">depick</span><span class="o">.</span><span class="n">load</span><span class="p">()</span>

<span class="c1"># recuperate the model and print the configurations</span>
<span class="n">vit_model</span> <span class="o">=</span> <span class="n">ViTForImageClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="n">model_name</span><span class="p">,</span>
    <span class="n">num_labels</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
    <span class="n">id2label</span> <span class="o">=</span> <span class="p">{</span><span class="n">name</span><span class="p">:</span> <span class="n">key</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">characs</span><span class="p">[</span><span class="s1">&#39;ids&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">items</span><span class="p">()},</span>
    <span class="n">label2id</span> <span class="o">=</span> <span class="n">characs</span><span class="p">[</span><span class="s1">&#39;ids&#39;</span><span class="p">]</span>
<span class="p">)</span>
<br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTForImageClassification: [&#39;pooler.dense.weight&#39;, &#39;pooler.dense.bias&#39;]
- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: [&#39;classifier.weight&#39;, &#39;classifier.bias&#39;]
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</pre></div></div>
</div>
<p>We loaded above the main configurations of the ViT Model understanding:</p>
<ul class="simple">
<li><p>A <code class="docutils literal notranslate"><span class="pre">hidden</span> <span class="pre">size</span></code> or more commonly <code class="docutils literal notranslate"><span class="pre">d_model</span></code> of 768</p></li>
<li><p>A <code class="docutils literal notranslate"><span class="pre">number</span> <span class="pre">of</span> <span class="pre">features</span> <span class="pre">of</span> <span class="pre">the</span></code>Feed Forward Network` of 3072</p></li>
<li><p>A <code class="docutils literal notranslate"><span class="pre">number</span> <span class="pre">of</span> <span class="pre">encoder</span> <span class="pre">and</span> <span class="pre">decoder</span> <span class="pre">layers</span></code> of 12</p></li>
<li><p>No drop-out is used, and a convolution layer of 3 input channels is used to provide the projection of the patches directly.</p></li>
</ul>
<p>This version of the ViT Model uses the convolution layer to recuperate the projected version of the patches. We no longer need to split the images into patches and transform each linearly. The size of the convolution layer‚Äôs output can be found as follows:</p>
<p><span class="math notranslate nohighlight">\(Out\_Height = Out\_Width = \frac{(224 - 16)}{16} + 1 = 14\)</span></p>
<p>And the number of channels of the convolution layer is equal to the <code class="docutils literal notranslate"><span class="pre">hidden</span> <span class="pre">size.</span></code> Then the number of channels of the images passes from <span class="math notranslate nohighlight">\(3 \rightarrow 768\)</span>. Including the number of channels, we obtain a final output size of <span class="math notranslate nohighlight">\((14, 14, 768)\)</span> for the embedding matrix. The embedding matrix is then transformed into a vector of dimension <span class="math notranslate nohighlight">\((14 \times 14, 768)\)</span> to add to a position embedding of the same size and fed into the ViT Model.</p>
<p>The id map that was provided earlier will be used to predict the test set.</p>
<p>The main differences between the original Transformer and the current Transformer are that the, later on, uses a convolution layer to obtain the projections, only an encoder stack is required, a pre-layer normalization is used in place of a post-layer normalization, a final <code class="docutils literal notranslate"><span class="pre">Multi-Layer</span> <span class="pre">Perceptron</span></code> (MLP) is added to classify the images from the outputs or state of the encoder and each feed-forward network use as activation function the <code class="docutils literal notranslate"><span class="pre">GeLU</span></code> in place of the <code class="docutils literal notranslate"><span class="pre">ReLU</span></code> activation. The <code class="docutils literal notranslate"><span class="pre">GeLU</span></code>
activation can be approximated by</p>
<div class="math notranslate nohighlight">
\[GeLU(x) = 0.5x(1 + tanh[\sqrt{2\pi}(x + 0.044715x^3)])\]</div>
<p>where <span class="math notranslate nohighlight">\(x\)</span> is the input to the <span class="math notranslate nohighlight">\(GeLU\)</span> activation. It provides a smoother function than the <code class="docutils literal notranslate"><span class="pre">ReLU.</span></code></p>
<p>Let us display the whole architecture below.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">vit_model</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
ViTForImageClassification(
  (vit): ViTModel(
    (embeddings): ViTEmbeddings(
      (patch_embeddings): ViTPatchEmbeddings(
        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
      )
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder): ViTEncoder(
      (layer): ModuleList(
        (0): ViTLayer(
          (attention): ViTAttention(
            (attention): ViTSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
            (output): ViTSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (intermediate): ViTIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ViTOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
        (1): ViTLayer(
          (attention): ViTAttention(
            (attention): ViTSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
            (output): ViTSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (intermediate): ViTIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ViTOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
        (2): ViTLayer(
          (attention): ViTAttention(
            (attention): ViTSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
            (output): ViTSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (intermediate): ViTIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ViTOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
        (3): ViTLayer(
          (attention): ViTAttention(
            (attention): ViTSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
            (output): ViTSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (intermediate): ViTIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ViTOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
        (4): ViTLayer(
          (attention): ViTAttention(
            (attention): ViTSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
            (output): ViTSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (intermediate): ViTIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ViTOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
        (5): ViTLayer(
          (attention): ViTAttention(
            (attention): ViTSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
            (output): ViTSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (intermediate): ViTIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ViTOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
        (6): ViTLayer(
          (attention): ViTAttention(
            (attention): ViTSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
            (output): ViTSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (intermediate): ViTIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ViTOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
        (7): ViTLayer(
          (attention): ViTAttention(
            (attention): ViTSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
            (output): ViTSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (intermediate): ViTIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ViTOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
        (8): ViTLayer(
          (attention): ViTAttention(
            (attention): ViTSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
            (output): ViTSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (intermediate): ViTIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ViTOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
        (9): ViTLayer(
          (attention): ViTAttention(
            (attention): ViTSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
            (output): ViTSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (intermediate): ViTIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ViTOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
        (10): ViTLayer(
          (attention): ViTAttention(
            (attention): ViTSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
            (output): ViTSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (intermediate): ViTIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ViTOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
        (11): ViTLayer(
          (attention): ViTAttention(
            (attention): ViTSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
            (output): ViTSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (intermediate): ViTIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ViTOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
  )
  (classifier): Linear(in_features=768, out_features=2, bias=True)
)
</pre></div></div>
</div>
</section>
<section id="Attention">
<h2>Attention<a class="headerlink" href="#Attention" title="Permalink to this heading">ÔÉÅ</a></h2>
<p>An attention matrix is provided by the Multi-head-attention of the last encoder layer and corresponds to the attention that was computed on each pixel of the image. It corresponds to two main series of concatenations between sub-attentions calculated inside the Multi-Head Attentions. Inside each Multi-Head Attention, we have Self-attention, which takes some linear transformations from the input sequence: the queries <span class="math notranslate nohighlight">\(q\)</span>, the values <span class="math notranslate nohighlight">\(v\)</span>, and the keys <span class="math notranslate nohighlight">\(k\)</span>. It then computes the
correlation between the queries and the keys as follows:</p>
<div class="math notranslate nohighlight">
\[attention_1 = \frac{q \times k^T}{\sqrt{hidden\_size}}\]</div>
<p>The correlation matrix of dimension <span class="math notranslate nohighlight">\((s_1, s_2)\)</span>, where <span class="math notranslate nohighlight">\(s_1 = s_2 = s\)</span> is the sequence length, is transformed into a matrix of probabilities defining the attention weight each element in sequence gives to others. It is obtained with the softmax function applied on the second dimension of the correlation matrix:</p>
<div class="math notranslate nohighlight">
\[attention_2 = \frac{\exp(-attention_1)}{\sum_{s_2} \exp(-attention_1)}\]</div>
<p>The final attention, which defines the most critical pixels in an image, is calculated by multiplying the attention weights of an element by each of their corresponding part of the sequence, summing over the results of that multiplications and concatenating all the summations made (each provides a vector of dimension <span class="math notranslate nohighlight">\((s, d_v)\)</span> where <span class="math notranslate nohighlight">\(d_v = d_k = d_q\)</span> is the dimension of the values and is equal to that of the keys and queries):</p>
<div class="math notranslate nohighlight">
\[attention_h = attention_2 \times v\]</div>
<p>The <span class="math notranslate nohighlight">\(h\)</span> index indicates that we calculate the attention of a <span class="math notranslate nohighlight">\(h^{th}\)</span> head. The concatenation between all the attentions is fed to the rest of the encoder layer process.</p>
<p>To visualize the attention, we will recuperate the output attention after the softmax function (<span class="math notranslate nohighlight">\(attention_2\)</span>) of the last encoder layer of dimension <span class="math notranslate nohighlight">\((batch\_size, num\_heads, s, s)\)</span> and take the attention provided by the previous element to each element in the sequence that we need to reshape to <span class="math notranslate nohighlight">\((batch\_size, num\_heads, patch\_size \times patch\_size)\)</span> (Notice that we didn‚Äôt add the batch size in the attention dimensions to simplify the explanation). The attention is
resized to match the original image and visualized with <code class="docutils literal notranslate"><span class="pre">matplotlib.</span></code> Notice that the first dimension is of size <span class="math notranslate nohighlight">\(num\_heads\)</span>; we must then take the average over that dimension to obtain a final attention matrix of dimension <span class="math notranslate nohighlight">\((batch\_size, 1, image\_height, image\_width)\)</span>. It will be multiplied with the original image for more precise visualization. Let us make an example with a random image from the dataset.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># take the first training image without shuffling</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s1">&#39;data/real_and_fake_face/training_fake/easy_1_1110.jpg&#39;</span><span class="p">)</span>

<span class="n">img</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_vit_model_11_0.png" src="../_images/notebooks_vit_model_11_0.png" />
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># recuperate the feature extractor</span>
<span class="n">feature_extractor</span> <span class="o">=</span> <span class="n">ViTFeatureExtractor</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>

<span class="c1"># recuperate the dataset</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">FakeFaceDetectionDataset</span><span class="p">(</span><span class="s1">&#39;data/real_and_fake_face/training_fake/&#39;</span><span class="p">,</span>
                                   <span class="s1">&#39;data/real_and_fake_face/training_real/&#39;</span><span class="p">,</span>
                                   <span class="n">id_map</span><span class="o">=</span><span class="n">characs</span><span class="p">[</span><span class="s1">&#39;ids&#39;</span><span class="p">],</span> <span class="n">transformer</span><span class="o">=</span><span class="n">feature_extractor</span><span class="p">,</span>
                                   <span class="n">transformer_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;return_tensors&#39;</span><span class="p">:</span> <span class="s1">&#39;pt&#39;</span><span class="p">})</span>

<span class="c1"># load a batch of one sequence</span>
<span class="n">image</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<p>Let us provide the pixel values and labels to the model and set output attentions to True to recuperate the attentions.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">outputs</span> <span class="o">=</span> <span class="n">vit_model</span><span class="p">(</span><span class="n">image</span><span class="p">[</span><span class="s1">&#39;pixel_values&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">output_attentions</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">image</span><span class="p">[</span><span class="s1">&#39;labels&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<p>Let us recuperate the attention matrix of the last encoder layer, take that of the previous patch and calculate the mean attention over the heads.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># recuperate the attention of the last encoder layer attention heads</span>
<span class="n">attention</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">attentions</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>

<span class="c1"># recuperate the attention provided by the last patch (notice that we eliminate 1 because of the +1 added by the convolutation layer)</span>
<span class="n">attention</span> <span class="o">=</span> <span class="n">attention</span><span class="p">[:,</span> <span class="p">:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">:]</span>

<span class="c1"># calculate the mean attention</span>
<span class="n">attention</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># let us recuperate the size of the original image and define patch size</span>
<span class="n">pixel_values</span> <span class="o">=</span> <span class="n">image</span><span class="p">[</span><span class="s1">&#39;pixel_values&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
<span class="n">img_size</span> <span class="o">=</span> <span class="n">pixel_values</span><span class="o">.</span><span class="n">shape</span>
<span class="n">patch_size</span> <span class="o">=</span> <span class="mi">14</span>

<span class="c1"># let us reshape transform the image to a numpy array</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">transforms</span><span class="o">.</span><span class="n">Resize</span><span class="p">(</span><span class="mi">224</span><span class="p">)(</span><span class="n">img</span><span class="p">))</span>

<span class="c1"># calculate the scale factor</span>
<span class="n">scale_factor</span> <span class="o">=</span> <span class="n">img_size</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">*</span> <span class="n">img_size</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">/</span> <span class="p">(</span><span class="n">patch_size</span> <span class="o">*</span> <span class="n">patch_size</span><span class="p">)</span>

<span class="c1"># rescale the attention with the nearest scaler</span>
<span class="n">attention</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">interpolate</span><span class="p">(</span><span class="n">attention</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">scale_factor</span><span class="o">=</span><span class="n">scale_factor</span><span class="p">,</span>
                          <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">)</span>

<span class="c1"># let us reshape the attention to the right size</span>
<span class="n">attention</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">img_size</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">img_size</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
<br/></pre></div>
</div>
</div>
<p>Let us multiply the obtained attention with the image and display the result.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># recuperate the result</span>
<span class="n">attention_image</span> <span class="o">=</span> <span class="n">img</span> <span class="o">/</span> <span class="mi">255</span> <span class="o">*</span> <span class="n">attention</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

<span class="c1"># visualize the result</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">attention_image</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_vit_model_18_0.png" src="../_images/notebooks_vit_model_18_0.png" />
</div>
</div>
<p>We obtained the above result because we still needed to train the model on the images. Let us create in the next section a function that makes us visualize the most critical pixels in the test set‚Äôs images.</p>
</section>
<section id="Metrics-and-predictions">
<h2>Metrics and predictions<a class="headerlink" href="#Metrics-and-predictions" title="Permalink to this heading">ÔÉÅ</a></h2>
<p>Let us create a function to compute the <code class="docutils literal notranslate"><span class="pre">accuracy,</span></code> the <code class="docutils literal notranslate"><span class="pre">f1-score</span></code>, and the <code class="docutils literal notranslate"><span class="pre">auc</span></code> at each training iteration.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%writefile</span> fake-face-detection/fake_face_detection/metrics/compute_metrics.py

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">evaluate</span>

<span class="n">metrics</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;f1&#39;</span><span class="p">:</span> <span class="n">evaluate</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;f1&#39;</span><span class="p">),</span>
    <span class="s1">&#39;accuracy&#39;</span><span class="p">:</span> <span class="n">evaluate</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;accuracy&#39;</span><span class="p">),</span>
    <span class="s1">&#39;roc_auc&#39;</span><span class="p">:</span> <span class="n">evaluate</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;roc_auc&#39;</span><span class="p">,</span> <span class="s1">&#39;multiclass&#39;</span><span class="p">)</span>
<span class="p">}</span>

<span class="k">def</span> <span class="nf">compute_metrics</span><span class="p">(</span><span class="n">p</span><span class="p">):</span> <span class="c1"># some part was got from https://huggingface.co/blog/fine-tune-vit</span>

    <span class="n">predictions</span><span class="p">,</span> <span class="n">label_ids</span> <span class="o">=</span> <span class="n">p</span>

    <span class="n">metric</span> <span class="o">=</span> <span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">),</span> <span class="n">references</span><span class="o">=</span><span class="n">label_ids</span><span class="p">)</span>

    <span class="n">f1_score</span> <span class="o">=</span> <span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;f1&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">),</span> <span class="n">references</span><span class="o">=</span><span class="n">label_ids</span><span class="p">)</span>

    <span class="n">metric</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">f1_score</span><span class="p">)</span>

    <span class="k">try</span><span class="p">:</span>

        <span class="n">auc</span> <span class="o">=</span> <span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;roc_auc&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">prediction_scores</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="n">references</span><span class="o">=</span><span class="n">label_ids</span><span class="p">)</span>

        <span class="n">metric</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">auc</span><span class="p">)</span>

    <span class="k">except</span><span class="p">:</span>

        <span class="k">pass</span>

    <span class="k">return</span> <span class="n">metric</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Overwriting fake-face-detection/fake_face_detection/metrics/compute_metrics.py
</pre></div></div>
</div>
<p>The following function will help us obtain the predictions, and the attention will be visualized in the tensorboard.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%writefile</span> fake-face-detection/fake_face_detection/metrics/make_predictions.py

<span class="kn">from</span> <span class="nn">fake_face_detection.data.fake_face_dataset</span> <span class="kn">import</span> <span class="n">FakeFaceDetectionDataset</span>
<span class="kn">from</span> <span class="nn">fake_face_detection.metrics.compute_metrics</span> <span class="kn">import</span> <span class="n">compute_metrics</span>
<span class="kn">from</span> <span class="nn">torch.utils.tensorboard</span> <span class="kn">import</span> <span class="n">SummaryWriter</span>
<span class="kn">from</span> <span class="nn">PIL.JpegImagePlugin</span> <span class="kn">import</span> <span class="n">JpegImageFile</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">transforms</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">glob</span> <span class="kn">import</span> <span class="n">glob</span>
<span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="k">def</span> <span class="nf">get_attention</span><span class="p">(</span><span class="n">image</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">JpegImageFile</span><span class="p">],</span> <span class="n">attention</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">size</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">,</span> <span class="n">patch_size</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">):</span>

    <span class="c1"># recuperate the image as a numpy array</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>

        <span class="k">with</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">image</span><span class="p">)</span> <span class="k">as</span> <span class="n">img</span><span class="p">:</span>

            <span class="n">img</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">transforms</span><span class="o">.</span><span class="n">Resize</span><span class="p">(</span><span class="n">size</span><span class="p">)(</span><span class="n">img</span><span class="p">))</span>

    <span class="k">else</span><span class="p">:</span>

        <span class="n">img</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">transforms</span><span class="o">.</span><span class="n">Resize</span><span class="p">(</span><span class="n">size</span><span class="p">)(</span><span class="n">image</span><span class="p">))</span>

    <span class="c1"># recuperate the attention provided by the last patch (notice that we eliminate 1 because of the +1 added by the convolutation layer)</span>
    <span class="n">attention</span> <span class="o">=</span> <span class="n">attention</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">:]</span>

    <span class="c1"># calculate the mean attention</span>
    <span class="n">attention</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># let us reshape transform the image to a numpy array</span>

    <span class="c1"># calculate the scale factor</span>
    <span class="n">scale_factor</span> <span class="o">=</span> <span class="n">size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">/</span> <span class="p">(</span><span class="n">patch_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">patch_size</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

    <span class="c1"># rescale the attention with the nearest scaler</span>
    <span class="n">attention</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">interpolate</span><span class="p">(</span><span class="n">attention</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">scale_factor</span><span class="o">=</span><span class="n">scale_factor</span><span class="p">,</span>
                            <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">)</span>

    <span class="c1"># let us reshape the attention to the right size</span>
    <span class="n">attention</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">size</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># recuperate the result</span>
    <span class="n">attention_image</span> <span class="o">=</span> <span class="n">img</span> <span class="o">/</span> <span class="mi">255</span> <span class="o">*</span> <span class="n">attention</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">attention_image</span>


<span class="k">def</span> <span class="nf">make_predictions</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">:</span> <span class="n">FakeFaceDetectionDataset</span><span class="p">,</span>
                     <span class="n">model</span><span class="p">,</span>
                     <span class="n">log_dir</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;fake_face_logs&quot;</span><span class="p">,</span>
                     <span class="n">tag</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;Attentions&quot;</span><span class="p">,</span>
                     <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
                     <span class="n">size</span><span class="p">:</span> <span class="nb">tuple</span> <span class="o">=</span> <span class="p">(</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">),</span>
                     <span class="n">patch_size</span><span class="p">:</span> <span class="nb">tuple</span> <span class="o">=</span> <span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">14</span><span class="p">),</span>
                     <span class="n">figsize</span><span class="p">:</span> <span class="nb">tuple</span> <span class="o">=</span> <span class="p">(</span><span class="mi">24</span><span class="p">,</span> <span class="mi">24</span><span class="p">)):</span>

    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>

        <span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

        <span class="c1"># initialize the logger</span>
        <span class="n">writer</span> <span class="o">=</span> <span class="n">SummaryWriter</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">log_dir</span><span class="p">,</span> <span class="s2">&quot;attentions&quot;</span><span class="p">))</span>

        <span class="c1"># let us recuperate the images and labels</span>
        <span class="n">images</span> <span class="o">=</span> <span class="n">test_dataset</span><span class="o">.</span><span class="n">images</span>

        <span class="n">labels</span> <span class="o">=</span> <span class="n">test_dataset</span><span class="o">.</span><span class="n">labels</span>

        <span class="c1"># let us initialize the predictions</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;attentions&#39;</span><span class="p">:</span> <span class="p">[],</span> <span class="s1">&#39;predictions&#39;</span><span class="p">:</span> <span class="p">[],</span> <span class="s1">&#39;true_labels&#39;</span><span class="p">:</span> <span class="n">labels</span><span class="p">,</span> <span class="s1">&#39;predicted_labels&#39;</span><span class="p">:</span> <span class="p">[]}</span>

        <span class="c1"># let us initialize the dataloader</span>
        <span class="n">test_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>

        <span class="c1"># get the loss</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">test_dataloader</span><span class="p">:</span>

            <span class="c1"># recuperate the pixel values</span>
            <span class="n">pixel_values</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;pixel_values&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

            <span class="c1"># recuperate the labels</span>
            <span class="n">labels_</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;labels&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

            <span class="c1"># # recuperate the outputs</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">pixel_values</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">labels_</span><span class="p">,</span> <span class="n">output_attentions</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>

            <span class="c1"># recuperate the predictions</span>
            <span class="n">predictions</span><span class="p">[</span><span class="s1">&#39;predictions&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">logits</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">(),</span> <span class="n">axis</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>

            <span class="c1"># recuperate the attentions of the last encoder layer</span>
            <span class="n">predictions</span><span class="p">[</span><span class="s1">&#39;attentions&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">attentions</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">())</span>

            <span class="c1"># add the loss</span>
            <span class="n">loss</span> <span class="o">+=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

        <span class="n">predictions</span><span class="p">[</span><span class="s1">&#39;predictions&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">predictions</span><span class="p">[</span><span class="s1">&#39;predictions&#39;</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>

        <span class="n">predictions</span><span class="p">[</span><span class="s1">&#39;attentions&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">predictions</span><span class="p">[</span><span class="s1">&#39;attentions&#39;</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>

        <span class="n">predictions</span><span class="p">[</span><span class="s1">&#39;predicted_labels&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">predictions</span><span class="p">[</span><span class="s1">&#39;predictions&#39;</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

        <span class="c1"># let us calculate the metrics</span>
        <span class="n">metrics</span> <span class="o">=</span> <span class="n">compute_metrics</span><span class="p">((</span><span class="n">predictions</span><span class="p">[</span><span class="s1">&#39;predictions&#39;</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">predictions</span><span class="p">[</span><span class="s1">&#39;true_labels&#39;</span><span class="p">])))</span>
        <span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_dataloader</span><span class="p">)</span>

        <span class="c1"># for each image we will visualize his attention</span>
        <span class="n">nrows</span> <span class="o">=</span> <span class="n">ceil</span><span class="p">(</span><span class="n">sqrt</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">images</span><span class="p">)))</span>

        <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="n">nrows</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="n">nrows</span><span class="p">,</span> <span class="n">figsize</span> <span class="o">=</span> <span class="n">figsize</span><span class="p">)</span>

        <span class="n">axes</span> <span class="o">=</span> <span class="n">axes</span><span class="o">.</span><span class="n">flat</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">images</span><span class="p">)):</span>

            <span class="n">attention_image</span> <span class="o">=</span> <span class="n">get_attention</span><span class="p">(</span><span class="n">images</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">predictions</span><span class="p">[</span><span class="s1">&#39;attentions&#39;</span><span class="p">][</span><span class="n">i</span><span class="p">],</span> <span class="n">size</span><span class="p">,</span> <span class="n">patch_size</span><span class="p">)</span>

            <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">attention_image</span><span class="p">)</span>

            <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Image </span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

            <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>

        <span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>

        <span class="p">[</span><span class="n">fig</span><span class="o">.</span><span class="n">delaxes</span><span class="p">(</span><span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">images</span><span class="p">),</span> <span class="n">nrows</span> <span class="o">*</span> <span class="n">nrows</span><span class="p">)]</span>

        <span class="n">writer</span><span class="o">.</span><span class="n">add_figure</span><span class="p">(</span><span class="n">tag</span><span class="p">,</span> <span class="n">fig</span><span class="p">)</span>

        <span class="c1"># let us remove the predictions and the attentions</span>
        <span class="k">del</span> <span class="n">predictions</span><span class="p">[</span><span class="s1">&#39;predictions&#39;</span><span class="p">]</span>
        <span class="k">del</span> <span class="n">predictions</span><span class="p">[</span><span class="s1">&#39;attentions&#39;</span><span class="p">]</span>

        <span class="c1"># let us recuperate the metrics and the predictions</span>
        <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">predictions</span><span class="p">),</span> <span class="n">metrics</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Overwriting fake-face-detection/fake_face_detection/metrics/make_predictions.py
</pre></div></div>
</div>
<p>Let us make predictions on the test dataset and print the metrics.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">run</span> fake-face-detection/fake_face_detection/metrics/make_predictions.py

<span class="c1"># recuperate the test dataset</span>
<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">FakeFaceDetectionDataset</span><span class="p">(</span><span class="s1">&#39;data/real_and_fake_splits/test/training_fake/&#39;</span><span class="p">,</span>
                                        <span class="s1">&#39;data/real_and_fake_splits/test/training_real/&#39;</span><span class="p">,</span>
                                        <span class="n">characs</span><span class="p">[</span><span class="s1">&#39;ids&#39;</span><span class="p">],</span>
                                        <span class="n">feature_extractor</span><span class="p">,</span> <span class="n">transformer_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;return_tensors&#39;</span><span class="p">:</span> <span class="s1">&#39;pt&#39;</span><span class="p">})</span>

<span class="c1"># recuperate the predictions</span>
<span class="n">predictions</span><span class="p">,</span> <span class="n">metrics</span> <span class="o">=</span> <span class="n">make_predictions</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">,</span> <span class="n">vit_model</span><span class="o">.</span><span class="n">cuda</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predictions</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>true_labels</th>
      <th>predicted_labels</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>200</th>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>201</th>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>202</th>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>203</th>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>204</th>
      <td>1</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
<p>205 rows √ó 2 columns</p>
</div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">metrics</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
{&#39;accuracy&#39;: 0.5707317073170731,
 &#39;f1&#39;: 0.66412213740458,
 &#39;loss&#39;: 0.6821330612984257}
</pre></div></div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="split_dataset.html" class="btn btn-neutral float-left" title="Split data ü´ß" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="deploy_to_hugging_face.html" class="btn btn-neutral float-right" title="Deploy the ViT Model üöÄ" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, Oumar Kane.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>