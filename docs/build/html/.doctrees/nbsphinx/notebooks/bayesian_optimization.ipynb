{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian Optimization from Scratch ðŸ”\n",
    "--------------------------\n",
    "\n",
    "We must create some functions in this notebook that will help us make Bayesian Optimization to search for the best hyperparameter that will give the best model on the validation set. Bayesian Optimization uses Gaussian Process to approximate the objective function given input data. In our case, the input data is the set of hyperparameter values we must tune. The Bayesian theorem is used to direct the search to find the minimum or maximum of our objective function. The objective function can be the `Accuracy,` the `Recall,` or whatever score function to evaluate the model performance. \n",
    "\n",
    "The Bayesian Theorem suggests identifying a prior distribution for the objective function and updating it with the likelihood obtained with data given the objective function to get a posterior distribution. The Gaussian Process (GP) is commonly used for noisy distribution, which is difficult to directly simple from. The GP doesn't need to optimize parameters to find the distribution and can give a standard derivation around the mean distribution, which defines the uncertainty about the approximation. \n",
    "\n",
    "The prior distribution is aMultivariate normal distribution with mean $m_0$ and variance $K_{x, x}$, which is commonly a kernel distribution like the Radial Basis function (RBF) and calculates the similarity between the input values. Some noise $\\epsilon$ are added to the Prior Distribution with a mean of 0 and a variance $\\sigma^2$. The posterior distribution is also Multivariate normal distribution for which we are searching the mean $m_y$ vector and the covariance matrix $\\Sigma$. The `Cholesty iterative method` is commonly used to find the best posterior mean and covariance for a given new point. We will explain further the Gaussian Process. \n",
    "\n",
    "For now, let us focus on Bayesian Optimization: \n",
    "\n",
    "- After finding the posterior distribution, we can obtain the mean value of the objective function for any group of new hyperparameters. The new objective function values are used to find the best new samples for the subsequent evaluation trial since we make trials before finding the best hyperparameter values.\n",
    "\n",
    "- The first trial finds the first score from the objective function given random hyperparameter values. \n",
    "\n",
    "- A `surrogate function` is used to give that score from the Posterior Multivariate Distribution of the objective function given the input data.\n",
    "\n",
    "- The estimated score from the `surrogate function` is then used in a new function named `acquisition function` to generate new samples from the hyperparameter's search spaces. It exists many different `acquisition functions.`\n",
    "\n",
    "- The new samples are concatenated to the previous ones and used to train a further Posterior distribution. \n",
    "\n",
    "- The process is repeated until finding the most satisfying score.\n",
    "\n",
    "**Note**:  This idea is related to reinforcement learning methods to search for the following action(s) which maximize the value function (the reward of long term). We can consider the value function to be the cumulative distribution function of the approximate Posterior Multivariate Distribution function over the new samples and the new actions to be the ones sampled from the value function plus a variance rate to explore more states. The states are abstracted (not visible) in our case."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement the Bayesian Optimization process from scratch since we want to customize it for the current project. We will not code the Gaussian Process Regression sub-process since it is already integrated into the `scikit-learn` library. We can also use the `GPytorch` library which can provide better result but for the purpose of that project we will not need it. Let us define nextly the objective function. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from functools import partial\n",
    "from torch.nn import MSELoss\n",
    "from scipy.stats import norm\n",
    "from functools import partial\n",
    "from typing import *\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `MSELoss` to calculate the value returned by the objective function, which is our training function. It will calculate the mean squared error between values predicted from a feed-forward neural network and pre-defined target values. The input and target values are randomly initialized from a Gaussian distribution. We will need input data of 8 variables and 100 samples (not very large, not fast up the training). The following parameters will be necessary for a sample example:\n",
    "\n",
    "- The number of epochs -> ... [1, 10]\n",
    "- The number of layers -> ... [1, 4]\n",
    "- The number of features -> ... [40, 100]\n",
    "- The learning rate -> ... [1e-1, 1e-4]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us initialize the input data and the targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn((100, 8))\n",
    "\n",
    "y = torch.rand((100, 1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us initialize the model and the objective function. Notice that we must add the noise we defined earlier to the final calculated score. The noise is sampled from a normal distribution with a mean of 0 and a scale that we must determine. Let us choose a $\\sigma^2 = 0.1$ scale as the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "def set_model(input_size: int = 8, n_features: int = 1, n_layers: int = 1):\n",
    "    \n",
    "    layers = [nn.Sequential(nn.Linear(input_size, n_features), nn.ReLU())]\n",
    "    \n",
    "    layers.extend([nn.Sequential(nn.Linear(n_features, n_features), nn.ReLU()) for i in range(n_layers - 1)])\n",
    "    \n",
    "    layers.append(nn.Sequential(nn.Linear(n_features, 1)))\n",
    "    \n",
    "    sequence = nn.Sequential(*layers)\n",
    "    \n",
    "    return sequence\n",
    "\n",
    "# Only one iteration will be sufficient\n",
    "def objective(optimizer: nn.Module, loss_fn: nn.Module, input: torch.Tensor, target: torch.Tensor, params: dict, scale: float = 0.1):\n",
    "    \n",
    "    noise = torch.distributions.normal.Normal(0.0, scale).sample().item()\n",
    "    \n",
    "    model = set_model(n_features=params['n_features'], n_layers=params['n_layers'])\n",
    "    \n",
    "    optimizer_ = optimizer(model.parameters(), lr = params['lr'])\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for _ in range(params['epochs']):\n",
    "        \n",
    "        outputs = model(input)\n",
    "        \n",
    "        loss = loss_fn(outputs, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer_.step()\n",
    "        \n",
    "        optimizer_.zero_grad()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    return 1 / np.mean(losses) + noise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must also define simple functions which generate random samples from search spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting fake-face-detection/fake_face_detection/utils/sampling.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile fake-face-detection/fake_face_detection/utils/sampling.py\n",
    "from typing import *\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def get_random_sample(search_space: dict, p: Union[List[float], None] = None):\n",
    "    \"\"\"Recuperate a random sample\n",
    "\n",
    "    Args:\n",
    "        search_space (dict): A dictionary defining the search space\n",
    "\n",
    "    Raises:\n",
    "        ValueError: 'min' and 'max' can only be numbers\n",
    "        KeyError: Only the following keys can be provided {'min', 'max'}, {'value'}, {'values'} or {'values', 'p'} \n",
    "\n",
    "    Returns:\n",
    "        Union[int, float, str]: The random sample \n",
    "    \"\"\"\n",
    "    \n",
    "    keys = set(search_space)\n",
    "    \n",
    "    if keys == set(['min', 'max']):\n",
    "        \n",
    "        assert search_space['min'] < search_space['max']\n",
    "        \n",
    "        if isinstance(search_space['min'], int) and isinstance(search_space['max'], int):\n",
    "            \n",
    "            return random.randint(search_space['min'], search_space['max'])\n",
    "        \n",
    "        elif isinstance(search_space['min'], float) or isinstance(search_space, float):\n",
    "            \n",
    "            return random.uniform(search_space['min'], search_space['max'])\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            raise ValueError(\"You can only provide int or float values with min max!\")\n",
    "    \n",
    "    elif keys == set(['value']):\n",
    "        \n",
    "        return search_space['value']\n",
    "    \n",
    "    elif keys.issubset(set(['values'])):\n",
    "        \n",
    "        p = None\n",
    "        \n",
    "        if 'p' in keys: p = search_space['p']\n",
    "        \n",
    "        return np.random.choice(search_space['values'], size = (1), p = p)[0]\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        raise KeyError(\"You didn't provide right keys! Try between: {'min', 'max'}, {'value'}, {'values'} or {'values', 'p'}\")\n",
    "        \n",
    "\n",
    "def get_random_samples(search_spaces: dict):\n",
    "    \"\"\"Recuperate random samples from a dictionary of search spaces\n",
    "\n",
    "    Args:\n",
    "        search_spaces (dict): A dictionary where the keys are the hyperparameter names and the values are the search spaces\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where the keys are the hyperparameter names and the values are the sampled values from the search spaces\n",
    "    \"\"\"\n",
    "    \n",
    "    samples = {}\n",
    "    \n",
    "    for search_space in search_spaces:\n",
    "        \n",
    "        samples[search_space] = get_random_sample(search_spaces[search_space])\n",
    "    \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run fake-face-detection/fake_face_detection/utils/sampling.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test the sampling functions and do training to obtain a first value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the search spaces\n",
    "search_spaces = {'epochs': {\n",
    "    'min': 1,\n",
    "    'max': 10\n",
    "    },\n",
    "    'n_layers': {\n",
    "        'values': [1, 2, 3, 4]\n",
    "    },\n",
    "    'n_features': {\n",
    "        'value': 50\n",
    "    },\n",
    "    'lr': {\n",
    "        'min': 1e-4,\n",
    "        'max': 1e-1\n",
    "    }\n",
    "}\n",
    "\n",
    "# recuperate random samples\n",
    "samples = get_random_samples(search_spaces)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the following samples for each hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epochs': 7, 'n_layers': 4, 'n_features': 50, 'lr': 0.02310174766808769}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us train the model with them and recuperate the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = objective(torch.optim.Adam, nn.MSELoss(), X, y, samples)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the following loss from the sampled hyperparameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.45991277150153"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now implement the surrogate function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The surrogate function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The surrogate tries to estimate the objective function using the Bayes theorem probabilistically. We want to find the probability of obtaining such a score $f$ conditionally to input data $D$. The score is the loss calculated after training, and the input data is the set of parameters. To simulate the posterior distribution $P(f/D)$, we decided to use the Gaussian Process (GP) Regression. The kernel used to calculate the similarity between the input data sample can be the `Radial Basis Function` kernel which commonly provides excellent results. The GP Regression is already implemented in sklearn. We can use it directly for our implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the distribution\n",
    "gp_model = GaussianProcessRegressor()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only data we have is the sample hyperparameter value which instantiates the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the input data\n",
    "data = [list(samples.values())]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the only score we currently have is the loss we calculated earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the scores\n",
    "scores = [[loss]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us fit the model with the data and scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GaussianProcessRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianProcessRegressor</label><div class=\"sk-toggleable__content\"><pre>GaussianProcessRegressor()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GaussianProcessRegressor()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gp_model.fit(data, scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can estimate the value of the target using the input data (and the standard deviation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, stds = gp_model.predict(data, return_std=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the following prediction, which is very close to the actual loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([5.45991277]), array([1.00000004e-05]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred, stds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is time to choose an acquisition function to generate new samples."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acquisition Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The acquisition function will use the surrogate to examine which of many random samples is the best suited for the next generation:\n",
    "\n",
    "1. First, we must generate random samples from the search spaces.\n",
    "2. Second, since we already have an approximation of the objective function's distribution conditionally to input data via the surrogate function, we can estimate the vector means and their corresponding standard deviations:\n",
    "$$\n",
    "\\mu_e, \\sigma_e \\sim P(f/samples)\n",
    "$$ \n",
    "3. Third, we must compare the values in the vector of the estimated means are compared to the best mean calculated as follows:\n",
    "$$\n",
    "\\max(\\mu), \\space where \\space \\mu,. \\sim P(f/input\\_data)$$ \n",
    "\n",
    "The probability of improvement is the cumulative normal distribution of the normalized distance between the estimated means and the best one (this distance represents the regret in RL). The probability of improvement is calculated as follows:\n",
    "\n",
    "$$\n",
    "PI = P(f < \\frac{\\mu_e - \\mu_{best}}{\\sigma_e + \\epsilon})\n",
    "$$\n",
    "\n",
    "Where $\\epsilon$ is added to avoid division by zero.\n",
    "\n",
    "**Remark**: it exists different acquisition functions like the UCB function. But for the purpose of our project we will focus on the probability of improvement.\n",
    "\n",
    "**Note**: In our example we want to minimize the loss so we must take $-\\mu_e$ and $-\\mu$ to find the best solution.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us implement bellow the acquisition function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting fake-face-detection/fake_face_detection/utils/acquisitions.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile fake-face-detection/fake_face_detection/utils/acquisitions.py\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from scipy.stats import norm\n",
    "from typing import *\n",
    "\n",
    "def PI_acquisition(X: List, X_prime: List, model: GaussianProcessRegressor, maximize: bool = True):\n",
    "    \"\"\"Acquisition function for bayesian optimization using probability of improvement\n",
    "\n",
    "    Args:\n",
    "        X (List): A list containing the input data\n",
    "        X_prime (List): A list containing the generate samples\n",
    "        model (GaussianProcessRegressor): The gaussian model to use\n",
    "        maximize (bool, optional): A boolean value indicating the optimization objective. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        List: A list containing the probabilities\n",
    "    \"\"\"\n",
    "    \n",
    "    # let us predict the means for the input data\n",
    "    mu = model.predict(X)\n",
    "    \n",
    "    # let us calculate the means and standard deviation for the random samples\n",
    "    mu_e, std_e = model.predict(X_prime, return_std=True)\n",
    "    \n",
    "    if not maximize:\n",
    "        \n",
    "        mu = -mu\n",
    "        \n",
    "        mu_e = -mu_e\n",
    "    \n",
    "    # let us take the best mean\n",
    "    mu_best = max(mu)\n",
    "    \n",
    "    # let us calculate the probability of improvement\n",
    "    probs = norm.cdf((mu_e - mu_best) / std_e)\n",
    "    \n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run fake-face-detection/fake_face_detection/utils/acquisitions.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next generated sample is the one which have the best probability of being chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting fake-face-detection/fake_face_detection/utils/generation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile fake-face-detection/fake_face_detection/utils/generation.py\n",
    "from fake_face_detection.utils.acquisitions import PI_acquisition\n",
    "from fake_face_detection.utils.sampling import get_random_samples\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from typing import *\n",
    "import numpy as np\n",
    "\n",
    "def PI_generate_sample(X: Iterable, model: GaussianProcessRegressor, search_spaces: dict, n_tests: int = 100, maximize: bool = True):\n",
    "    \"\"\"Generate new samples with the probability of improvement\n",
    "\n",
    "    Args:\n",
    "        X (Iterable): The list of input data\n",
    "        model (GaussianProcessRegressor): The model to train\n",
    "        search_spaces (dict): The search spaces\n",
    "        n_tests (int, optional): The number of random samples to test. Defaults to 100.\n",
    "        maximize (bool, optional): The optimization strategy. If maximize == True -> maximize, else -> minimize. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        List: The new sample\n",
    "    \"\"\"\n",
    "    \n",
    "    # let us create random samples\n",
    "    X_prime = [list(get_random_samples(search_spaces).values()) for i in range(n_tests)]\n",
    "    \n",
    "    # let us recuperate the probabilities from the acquisition function\n",
    "    probs = PI_acquisition(X, X_prime, model, maximize = maximize)\n",
    "    \n",
    "    # let us return the best sample\n",
    "    return X_prime[np.argmax(probs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run fake-face-detection/fake_face_detection/utils/generation.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us generate the next sample with the above function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_samples = PI_generate_sample(data, gp_model, search_spaces, maximize = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtained the following new values for the next training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 50, 0.03131119507929577]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_samples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us train again the model and recuperate the new score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the new dictionary of samples\n",
    "params = {key: new_samples[i] for i, key in enumerate(search_spaces)}\n",
    "\n",
    "# calculate the new score\n",
    "new_score = objective(torch.optim.Adam, nn.MSELoss(), X, y, params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.981966343907849"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have enough data so we can obtain a worth loss. We must concatenate the generated samples and scores in order to obtain a more accurate prediction from the surrogate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.append(new_samples)\n",
    "scores.append([new_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting fake-face-detection/fake_face_detection/optimization/bayesian_optimization.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile fake-face-detection/fake_face_detection/optimization/bayesian_optimization.py\n",
    "from fake_face_detection.utils.generation import PI_generate_sample as generate_sample\n",
    "from fake_face_detection.utils.acquisitions import PI_acquisition as acquisition\n",
    "from fake_face_detection.utils.sampling import get_random_samples\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from typing import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class SimpleBayesianOptimization:\n",
    "    \n",
    "    def __init__(self, objective: Callable, search_spaces: dict, maximize: bool = True):\n",
    "        \n",
    "        # recuperate the optimization strategy\n",
    "        self.maximize = maximize\n",
    "        \n",
    "        # recuperate random sample\n",
    "        sample = get_random_samples(search_spaces)\n",
    "        \n",
    "        # initialize the search spaces\n",
    "        self.search_spaces = search_spaces\n",
    "        \n",
    "        # initialize the objective function\n",
    "        self.objective = objective\n",
    "        \n",
    "        # calculate the first score\n",
    "        score = objective(sample)\n",
    "        \n",
    "        # initialize the model\n",
    "        self.model = GaussianProcessRegressor()\n",
    "        \n",
    "        # initialize the input data\n",
    "        self.data = [list(sample.values())]\n",
    "        \n",
    "        # initialize the scores\n",
    "        self.scores = [[score]]\n",
    "        \n",
    "        # fit the model with the input data and the target\n",
    "        self.model.fit(self.data, self.scores)\n",
    "    \n",
    "    def optimize(self, n_trials: int = 50, n_tests: int = 100):\n",
    "        \"\"\"Finding the best hyperparameters with the Bayesian Optimization\n",
    "\n",
    "        Args:\n",
    "            n_trials (int, optional): The number of trials. Defaults to 50.\n",
    "            n_tests (int, optional): The number of random samples to test for each trial. Defaults to 100.\n",
    "        \"\"\"\n",
    "        # let us make multiple trials in order to find the best params\n",
    "        for _ in range(n_trials):\n",
    "            \n",
    "            # let us generate new samples with the acquisition and the surrogate functions\n",
    "            new_sample = generate_sample(self.data, self.model, self.search_spaces, n_tests, maximize = self.maximize)\n",
    "            sample = {key: new_sample[i] for i, key in enumerate(self.search_spaces)}\n",
    "            \n",
    "            # let us recuperate a new score from the new sample\n",
    "            new_score = self.objective(sample)\n",
    "            \n",
    "            # let us add the new sample, target and score to their lists\n",
    "            self.data.append(new_sample)\n",
    "            \n",
    "            self.scores.append([new_score])\n",
    "            \n",
    "            # let us train again the model\n",
    "            self.model.fit(self.data, self.scores)\n",
    "        \n",
    "    def get_results(self):\n",
    "        \"\"\"Recuperate the generated samples and the scores\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: A data frame containing the results\n",
    "        \"\"\"\n",
    "        # let us return the results as a data frame\n",
    "        data = {key: np.array(self.data, dtype = object)[:, i] for i, key in enumerate(self.search_spaces)}\n",
    "        \n",
    "        data.update({'score': np.array(self.scores)[:, 0]})\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run fake-face-detection/fake_face_detection/optimization/bayesian_optimization.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us make 50 trials in order to find the set of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the attributes\n",
    "simple_bayesian_optimization = SimpleBayesianOptimization(partial(objective, torch.optim.Adam, nn.MSELoss(), X, y), search_spaces, maximize=False)\n",
    "\n",
    "# optimize to find the best hyperparameters\n",
    "simple_bayesian_optimization.optimize(50)\n",
    "\n",
    "# recuperate the results\n",
    "results = simple_bayesian_optimization.get_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epochs</th>\n",
       "      <th>n_layers</th>\n",
       "      <th>n_features</th>\n",
       "      <th>lr</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>0.047436</td>\n",
       "      <td>6.404869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>0.017284</td>\n",
       "      <td>5.858079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.014734</td>\n",
       "      <td>6.854856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>0.033547</td>\n",
       "      <td>5.269045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>0.055477</td>\n",
       "      <td>2.092933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.09402</td>\n",
       "      <td>-0.105739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.076977</td>\n",
       "      <td>0.254223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.059555</td>\n",
       "      <td>2.381605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>0.095012</td>\n",
       "      <td>0.248136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>0.096846</td>\n",
       "      <td>0.127560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.089688</td>\n",
       "      <td>-0.078562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>0.082321</td>\n",
       "      <td>1.490068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.094775</td>\n",
       "      <td>-0.000623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.044421</td>\n",
       "      <td>0.978622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.085948</td>\n",
       "      <td>0.061297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.081952</td>\n",
       "      <td>-0.012566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>0.042656</td>\n",
       "      <td>1.983545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.053319</td>\n",
       "      <td>0.799534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>0.004587</td>\n",
       "      <td>4.902961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>0.021009</td>\n",
       "      <td>7.128041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>0.079397</td>\n",
       "      <td>0.987655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.088559</td>\n",
       "      <td>-0.152193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.074723</td>\n",
       "      <td>0.013811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>0.09264</td>\n",
       "      <td>-0.224459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>0.08645</td>\n",
       "      <td>0.280591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.080311</td>\n",
       "      <td>0.240702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>0.09727</td>\n",
       "      <td>-0.012525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.091157</td>\n",
       "      <td>-0.001376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.099791</td>\n",
       "      <td>0.012959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.042588</td>\n",
       "      <td>0.999145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.079175</td>\n",
       "      <td>0.087425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.09686</td>\n",
       "      <td>0.184545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.071047</td>\n",
       "      <td>0.731057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.040592</td>\n",
       "      <td>2.449255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.013261</td>\n",
       "      <td>8.836807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.048035</td>\n",
       "      <td>1.844829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>0.085077</td>\n",
       "      <td>0.236993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.066092</td>\n",
       "      <td>0.690044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.063592</td>\n",
       "      <td>1.830456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>0.09657</td>\n",
       "      <td>0.017104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.095475</td>\n",
       "      <td>-0.000947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.088609</td>\n",
       "      <td>0.026383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.068641</td>\n",
       "      <td>1.021106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>0.059763</td>\n",
       "      <td>3.608931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.033989</td>\n",
       "      <td>3.485960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.089095</td>\n",
       "      <td>0.016238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.063018</td>\n",
       "      <td>0.136449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.051632</td>\n",
       "      <td>1.227410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>0.08359</td>\n",
       "      <td>0.065483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.07365</td>\n",
       "      <td>0.429036</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epochs n_layers n_features        lr     score\n",
       "0      10        1         50  0.047436  6.404869\n",
       "1       3        3         50  0.017284  5.858079\n",
       "2       7        4         50  0.014734  6.854856\n",
       "3       6        1         50  0.033547  5.269045\n",
       "4       1        1         50  0.055477  2.092933\n",
       "5      10        4         50   0.09402 -0.105739\n",
       "6      10        4         50  0.076977  0.254223\n",
       "7       1        4         50  0.059555  2.381605\n",
       "8      10        3         50  0.095012  0.248136\n",
       "9       9        3         50  0.096846  0.127560\n",
       "10      9        4         50  0.089688 -0.078562\n",
       "11      3        1         50  0.082321  1.490068\n",
       "12      9        4         50  0.094775 -0.000623\n",
       "13      9        4         50  0.044421  0.978622\n",
       "14      5        4         50  0.085948  0.061297\n",
       "15      9        4         50  0.081952 -0.012566\n",
       "16      5        3         50  0.042656  1.983545\n",
       "17      8        2         50  0.053319  0.799534\n",
       "18      8        1         50  0.004587  4.902961\n",
       "19      4        1         50  0.021009  7.128041\n",
       "20      2        1         50  0.079397  0.987655\n",
       "21      4        4         50  0.088559 -0.152193\n",
       "22      4        4         50  0.074723  0.013811\n",
       "23      8        3         50   0.09264 -0.224459\n",
       "24      8        3         50   0.08645  0.280591\n",
       "25      8        2         50  0.080311  0.240702\n",
       "26      8        3         50   0.09727 -0.012525\n",
       "27      6        4         50  0.091157 -0.001376\n",
       "28      7        4         50  0.099791  0.012959\n",
       "29      6        4         50  0.042588  0.999145\n",
       "30      7        4         50  0.079175  0.087425\n",
       "31      5        4         50   0.09686  0.184545\n",
       "32      8        2         50  0.071047  0.731057\n",
       "33      5        4         50  0.040592  2.449255\n",
       "34      8        2         50  0.013261  8.836807\n",
       "35     10        4         50  0.048035  1.844829\n",
       "36      7        3         50  0.085077  0.236993\n",
       "37      7        2         50  0.066092  0.690044\n",
       "38      1        2         50  0.063592  1.830456\n",
       "39      6        3         50   0.09657  0.017104\n",
       "40      8        4         50  0.095475 -0.000947\n",
       "41      3        4         50  0.088609  0.026383\n",
       "42      9        2         50  0.068641  1.021106\n",
       "43      1        3         50  0.059763  3.608931\n",
       "44      2        4         50  0.033989  3.485960\n",
       "45      4        4         50  0.089095  0.016238\n",
       "46      2        2         50  0.063018  0.136449\n",
       "47      3        2         50  0.051632  1.227410\n",
       "48      4        3         50   0.08359  0.065483\n",
       "49      6        2         50   0.07365  0.429036"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the results\n",
    "pd.options.display.max_rows = 50\n",
    "results.head(50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best loss and the corresponding hyperparameters are the followings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epochs</th>\n",
       "      <th>n_layers</th>\n",
       "      <th>n_features</th>\n",
       "      <th>lr</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>0.09264</td>\n",
       "      <td>-0.224459</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epochs n_layers n_features       lr     score\n",
       "23      8        3         50  0.09264 -0.224459"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[results['score'] == results['score'].min()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us get the loss obtained with best set of hyperparameters without adding a large noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06265220941131512"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recuperate the parameters\n",
    "params = results[results['score'] == results['score'].min()]\n",
    "\n",
    "params = params.drop('score', axis = 1).to_dict('list')\n",
    "\n",
    "params = {key: value[0] for key, value in params.items()}\n",
    "\n",
    "# train and get the loss\n",
    "objective(torch.optim.Adam, nn.MSELoss(), X, y, params, scale=1e-5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final loss is equal to `0.063`. The first loss was equal to `6.40`. We highly progressed since the first random sample âœŒï¸."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch1-HleOW5am-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
