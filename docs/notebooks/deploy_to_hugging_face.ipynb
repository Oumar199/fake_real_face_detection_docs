{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy the ViT Model ðŸš€\n",
    "-------------------------\n",
    "\n",
    "We will use `streamlit` in this notebook to create our small application with the best model. The application will then be deployed to Hugging Face with `git.` However, we will only focus on the creation part in the notebook. Refer to the following Tutorial [deploy_to_hugging_face](https://huggingface.co/blog/streamlit-spaces) for the Hugging Face's deployment configuration. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Streamlit is an open-source Python framework that makes it simple to understand and visualize applications mixed with machine learning. It provides a customizable design, but we will use the default structure and stylesheets for the project. We need to add the following parts to our application:\n",
    "\n",
    "- We will need to define a file uploader to recuperate an image from a local directory\n",
    "- The name of the file and the image must be visualized \n",
    "- A button will be used to submit the image to the model\n",
    "- The prediction will be printed close to the image "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A unique file can be used to create all of the parts of the application. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "from transformers import ViTForImageClassification, ViTFeatureExtractor\n",
    "from fake_face_detection.metrics.make_predictions import get_attention\n",
    "from torchvision import transforms\n",
    "import streamlit as st\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import cv2\n",
    "\n",
    "# set the color of the header\n",
    "def header(text):\n",
    "    st.markdown(f\"<h1 style = 'color: #4B4453; text-align: center'>{text}</h1>\", unsafe_allow_html=True)\n",
    "    st.markdown(\"\"\"---\"\"\")\n",
    "\n",
    "# initialize the size\n",
    "size = (224, 224)\n",
    "\n",
    "# let us add a header\n",
    "header(\"FAKE AND REAL FACE DETECTION\")\n",
    "\n",
    "# let us add an expander to write some description of the application\n",
    "expander = st.expander('Description', expanded=True)\n",
    "\n",
    "with expander:\n",
    "    st.write('''This is a long text lorem ipsum dolor''')\n",
    "\n",
    "# let us initialize two columns\n",
    "left, mid, right = st.columns(3)\n",
    "\n",
    "# the following function will load the model (must be in cache)\n",
    "@st.cache_resource\n",
    "def get_model():\n",
    "    \n",
    "    # let us load the image characteristics\n",
    "    with open('data/extractions/fake_real_dict.txt', 'rb') as f:\n",
    "        \n",
    "        depick = pickle.Unpickler(f)\n",
    "        \n",
    "        characs = depick.load()\n",
    "    \n",
    "    # define the model name\n",
    "    model_name = 'google/vit-base-patch16-224-in21k'\n",
    "    \n",
    "    # recuperate the model\n",
    "    model = ViTForImageClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels = len(characs['ids']),\n",
    "        id2label = {name: key for key, name in characs['ids'].items()},\n",
    "        label2id = characs['ids']\n",
    "    )\n",
    "    \n",
    "    # recuperate the feature_extractor\n",
    "    feature_extractor = ViTFeatureExtractor(model_name)\n",
    "    \n",
    "    return model, feature_extractor\n",
    "\n",
    "# let us add a file uploader\n",
    "st.subheader(\"Choose an image to inspect\")\n",
    "file = st.file_uploader(\"\", type='jpg')\n",
    "\n",
    "# if the file is correctly uploaded make the next processes\n",
    "if file is not None:\n",
    "    \n",
    "    # convert the file to an opencv image\n",
    "    file_bytes = np.asarray(bytearray(file.read()), dtype=np.uint8)\n",
    "    \n",
    "    opencv_image = cv2.imdecode(file_bytes, 1)\n",
    "    \n",
    "    # resize the image\n",
    "    opencv_image = cv2.resize(opencv_image, size)\n",
    "    \n",
    "    # Let us display the image\n",
    "    left.header(\"Loaded image\")\n",
    "    \n",
    "    left.image(opencv_image, channels='BGR')\n",
    "    \n",
    "    left.markdown(\"\"\"---\"\"\")\n",
    "    \n",
    "    if left.button(\"SUBMIT\"):\n",
    "        \n",
    "        # Let us convert the image format to 'RGB'\n",
    "        image = cv2.cvtColor(opencv_image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Let us convert from opencv image to pil image\n",
    "        image = Image.fromarray(image)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # Recuperate the model and the feature extractor\n",
    "            model, feature_extractor = get_model()\n",
    "            \n",
    "            # Change to evaluation mode\n",
    "            _ = model.eval()\n",
    "            \n",
    "            # Apply transformation on the image\n",
    "            image_ = feature_extractor(image, return_tensors = 'pt')\n",
    "            \n",
    "            # # Recuperate output from the model\n",
    "            outputs = model(image_['pixel_values'], output_attentions = True)\n",
    "            \n",
    "            # Recuperate the predictions\n",
    "            predictions = torch.argmax(outputs.logits, axis = -1)\n",
    "            \n",
    "            # Write the prediction to the middle\n",
    "            mid.markdown(f\"<h2 style='text-align: center; padding: 2cm; color: black; background-color: orange; border: darkorange solid 0.3px; box-shadow: 0.2px 0.2px 0.6px 0.1px gray'>{model.config.id2label[predictions[0].item()]}</h2>\", unsafe_allow_html=True)\n",
    "            \n",
    "            # Let us recuperate the attention\n",
    "            attention = outputs.attentions[-1][0]\n",
    "            \n",
    "            # Let us recuperate the attention image\n",
    "            attention_image = get_attention(image, attention, size = (224, 224), patch_size = (14, 14))\n",
    "\n",
    "            # Let us transform the attention image to a opencv image\n",
    "            attention_image = cv2.cvtColor(attention_image.astype('float32'), cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            # Let us display the attention image\n",
    "            right.header(\"Attention\")\n",
    "            \n",
    "            right.image(attention_image, channels='BGR')\n",
    "            \n",
    "            right.markdown(\"\"\"---\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Fake-face-detection` is now accessible at the following link [Fake_Real_Face_Detection_App](https://huggingface.co/spaces/Oumar199/Fake-Real-Face-Detection) ðŸš€.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch1-HleOW5am-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
