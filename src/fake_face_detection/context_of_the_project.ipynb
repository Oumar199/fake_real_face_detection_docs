{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real and Fake Face Detection (RFFD)\n",
    "---------------------------\n",
    "\n",
    "<p style=\"text-align: justify\">\n",
    "In this project, we use Deep Neural Networks to identify which image is fake or real. The training will be done on a dataset that we got from Kaggle (check it here <a href=\"https://www.kaggle.com/datasets/ciplab/real-and-fake-face-detection?resource=download)\">kaggle_real_fake_faces</a>) created by <i style=\"color:chocolate\">Seonghyeon Nam, Seoung Wug Oh, et al.</i> They used expert knowledge to photoshop authentic images. The fake images range between easy, medium, or hard to recognize. The modifications are made on the eyes, nose, and mouth (which permit human beings to recognize others) or the whole face.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fake_photoshop](https://github.com/minostauros/Real-and-Fake-Face-Detection/raw/master/filename_description.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i style=\"color: lightgray\">The above image was got from the kaggle description of the image and describe  a file.</i>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image above is described as a fake image file. The name of the file can be decomposed into three different parts separated by underscores:\n",
    "\n",
    "- The first part indicates the quality of the Photoshop or the difficulty of recognizing that it is fake;\n",
    "- The second part indicates the identification number of the image;\n",
    "- The third and final part indicates the modified segment of the face in binary digits with the following signature -> \n",
    "```json\n",
    "[bit_left_eye, bit_right_eye, bit_nose, bit_mouth]\n",
    "```\n",
    "\n",
    "The segment is modified if it is the positive bit (1). Otherwise, the segment is not modified. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define bellow, with more details, our main objective."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of the project is to use `Vision Transformer (ViT)` mixed with `Transfer Learning` to achieve a great accuracy and recall on the validation set. ViT is a new field which try to reproduce the same performance that the Convolution Neural Networks on image classification task but using the Transformer architecture. It can provide very accurate results. \n",
    "\n",
    "![VISION_TRANSFORMER](https://ghost.graviti.com/content/images/size/w1000/2022/02/image-1.png)\n",
    "\n",
    "However, we cannot obtain such great result with only few images. ViT require around 14 millions images to learn on image classification task and we want to train the model only on one GPU device. Then the solution is to use Transfer Learning with a pre-trained Transformer to improve the overall performance. \n",
    "\n",
    "We will fine-tune the pre-trained ViT Model for which the ArXiv paper is available at the following link [VisualBert](https://arxiv.org/pdf/1908.03557.pdf). It was pre-trained on the ImageNet-21k which contains 14 millions of images distributed over 21 thousand classes. The model is available in HuggingFace and can be import with the HuggingFace API.\n",
    "\n",
    "For the moment, we want to obtain the following scores on the validation set:\n",
    "\n",
    "- **Accuracy > 80**\n",
    "- **f1 > 80**\n",
    "\n",
    "Since it is related to medical predictions but only on social network security managing we will enforce ourself to obtain not search for more than 90% of **Accuracy** and **f1-score**.\n",
    "\n",
    "The next section describe the steps that are required to achieve the project."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps üßæ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define bellow the main parts of our project:\n",
    "\n",
    "- Data generation and exploration: We must recuperate the images and visualize them and identify their statistics. Moreover we will identify the augmentation methods to add to the images ‚û°Ô∏è [Generating_and_visualizing](generate_and_visualize.ipynb)\n",
    "\n",
    "- Preprocessing method: We must after exploration define the preprocessing to add before training the model on them. ‚û°Ô∏è [Preprocessing_and_loading](preprocessing_and_loading.ipynb)\n",
    "\n",
    "- Split the images between train, validation and test sets. ‚û°Ô∏è [Data_splitting](split_dataset.ipynb)\n",
    "\n",
    "- Load the ViT Model, explain briefly the architecture and define the metrics to add. ‚û°Ô∏è [VitModel_Metrics](vit_model.ipynb)\n",
    "\n",
    "- Search for the best model with The Bayesian Optimization strategy.\n",
    "\n",
    "- Fine-tune the best model.\n",
    "\n",
    "- Evaluate the model on the test set.\n",
    "\n",
    "- Deploy the model to Hugging Face."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
